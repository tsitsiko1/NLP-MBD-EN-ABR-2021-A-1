{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parts of Speech Tagging Practice\n",
    "\n",
    "The purpose of this practical session is to experiment with Part-of-Speech tagging, using the tools provided by NLTK. \n",
    "\n",
    "We will make use of the contents of the [Chapter 5](http://www.nltk.org/book/ch05.html) of the \n",
    "[Natural Language Processing with Python --- Analyzing Text with the Natural Language Toolkit](http://www.nltk.org/book). As experimental dataset, we will use the [Brown Corpus](http://en.wikipedia.org/wiki/Brown_Corpus). The Brown Corpus defines a tagset (specific collection of part-of-speech labels) that has been reused in many other annotated resources in English. The [universal tagset](http://universaldependencies.org/u/pos/) includes 17 tags:\n",
    "\n",
    "Tag\t| Meaning\t | Examples\n",
    "----|------------|----------\n",
    "ADJ\t| adjective\t | new, good, high, special, big, local\n",
    "ADV\t| adverb\t | really, already, still, early, now\n",
    "CONJ| conjunction| and, or, but, if, while, although\n",
    "DET\t| determiner | the, a, some, most, every, no\n",
    "X\t| other, foreign words | dolce, ersatz, esprit, quo, maitre\n",
    "NOUN | noun\t     | year, home, costs, time, education\n",
    "PROPN| proper noun | Alison, Africa, April, Washington\n",
    "NUM\t | numeral\t| twenty-four, fourth, 1991, 14:24\n",
    "PRON | pronoun\t| he, their, her, its, my, I, us\n",
    "ADP  | adposition, preposition | on, of, at, with, by, into, under\n",
    "AUX\t | auxiliary verb | has (done), is (doing), will (do), should (do), must (do), can (do)\n",
    "INTJ | interjection | ah, bang, ha, whee, hmpf, oops\n",
    "VERB | verb | is, has, get, do, make, see, run\n",
    "PART | particle | possessive marker 's, negation 'not'\n",
    "SCONJ | subordinating conjunction: complementizer, adverbial clause introducer | I believe 'that' he will come, if, while\n",
    "SYM\t| symbol | $, (C), +, *, /, =, :), john.doe@example.com\n",
    "\n",
    "\n",
    "\n",
    "Note that the decision on how to tag a word, without more information is ambiguous for multiple reasons:\n",
    "\n",
    "- The same string can be understood as a `noun` or a `verb` (e.g, **book**).\n",
    "- Some POS tags have a systematically ambiguous definition: a present participle can be used in progressive verb usages (I am going:VERB), but it can also be used in an adjectival position modifying a noun: (A striking:ADJ comparison). In other words, it is unclear in the definition itself of the tag whether the tag refers to a syntactic function or to a morphological property of the word.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Working on the Brown Corpus with NLTK\n",
    "\n",
    "NLTK contains a collection of tagged corpora, arranged as convenient Python objects. We will use the `Brown corpus` in this experiment.  The `tagged_sents` version of the corpus is a list of sentences. Each sentence is a list of pairs (`tuples`) `(word, tag)`. Similarly, one can access the corpus as a flat list of tagged words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\madcastea\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\brown.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download and import the Brown Corpus\n",
    "nltk.download('brown')\n",
    "from nltk.corpus import brown\n",
    "\n",
    "brown_news_tagged = brown.tagged_sents(categories='news', tagset='universal')\n",
    "brown_news_words = brown.tagged_words(categories='news',  tagset='universal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the `Brown Corpus` we also need to also load the `universal_tagset`, an interface for converting POS tags from various formats to the universal tagset format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\madcastea\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\universal_tagset.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring success: Accuracy, Training Dataset, Test Dataset\n",
    "\n",
    "Assume we develop a tagger. How do we know how successful it is? Can we trust the decisions the tagger makes? In order to evaluate the tagger, we are going to split the dataset into training and testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagged:  [('The', 'DET'), ('Fulton', 'NOUN'), ('County', 'NOUN'), ('Grand', 'ADJ'), ('Jury', 'NOUN'), ('said', 'VERB'), ('Friday', 'NOUN'), ('an', 'DET'), ('investigation', 'NOUN'), ('of', 'ADP'), (\"Atlanta's\", 'NOUN'), ('recent', 'ADJ'), ('primary', 'NOUN'), ('election', 'NOUN'), ('produced', 'VERB'), ('``', '.'), ('no', 'DET'), ('evidence', 'NOUN'), (\"''\", '.'), ('that', 'ADP'), ('any', 'DET'), ('irregularities', 'NOUN'), ('took', 'VERB'), ('place', 'NOUN'), ('.', '.')]\n",
      "\n",
      "Untagged:  ['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.']\n"
     ]
    }
   ],
   "source": [
    "brown_train = brown_news_tagged[100:]\n",
    "brown_test = brown_news_tagged[:100]\n",
    "\n",
    "from nltk.tag import untag\n",
    "test_sent = untag(brown_test[0])\n",
    "print(\"Tagged: \", brown_test[0])\n",
    "print()\n",
    "print(\"Untagged: \", test_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Baseline Tagger: Default Tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the absence of any knowledge, the most basic tagging approach is to assign the same tag to all the words.\n",
    "It can be done with the `DefaultTagger` class, which takes a tag and assign it to all the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', 'default_tag'),\n",
       " ('is', 'default_tag'),\n",
       " ('a', 'default_tag'),\n",
       " ('test', 'default_tag')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A default tagger assigns the same tag to all words\n",
    "from nltk import DefaultTagger\n",
    "default_tagger = DefaultTagger('default_tag')\n",
    "default_tagger.tag('This is a test'.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `DefaultTagger`, try different tags (see the available options in the table at the beginning of the notebook).\n",
    "\n",
    "**Which one is offering the best performance? Why?**\n",
    "\n",
    "To measure success, in this task, we will measure accuracy. The tagger object in NLTK includes a method called `evaluate` to measure the accuracy of a tagger on a given test set (our `brown_test` object).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try different tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sources of Knowledge to Improve Tagging Accuracy\n",
    "\n",
    "Intuitively, the sources of knowledge that can help us decide what is the tag of a word include:\n",
    "- A dictionary that lists the possible parts of speech for each word\n",
    "- The context of the word in a sentence (neighboring words)\n",
    "- The morphological form of the word (suffixes, prefixes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Lookup Tagger: Using Dictionary Knowledge\n",
    "\n",
    "Assume we have a dictionary that lists the possible tags for each word in English. Could we use this information to perform better tagging?\n",
    "\n",
    "The intuition is that we would only assign to a word a tag that it can have in the dictionary. For example, if `box` can only be a `Verb` or a `Noun`, when we have to tag an instance of the word `box`, we only choose between 2 options - and not between 17 options.\n",
    "\n",
    "There are 3 issues we must address to turn this into working code:\n",
    "\n",
    "- Where do we get the dictionary?\n",
    "- How do we choose between the various tags associated to a word in the dictionary? (For example, how do we choose between `VERB` and `NOUN` for `box`).\n",
    "- What do we do for words that do not appear in the dictionary?\n",
    "\n",
    "The simple solutions we will test are the following - note that for each question, there exist other strategies that we will investigate later:\n",
    "\n",
    "- Where do we get the dictionary?: we will learn it from a sample dataset.\n",
    "- How do we choose between the various tags associated to a word in the dictionary?: we will choose the most likely tag as observed in the sample dataset.\n",
    "- What do we do for words that do not appear in the dictionary?: we will pass unknown words to a backoff tagger (tag all unknown words as `NOUN`).\n",
    "\n",
    "The `nltk.UnigramTagger` implements this overall strategy. It must be trained on a dataset, from which it builds a model of \"unigrams\". The following code shows how it is used:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.1.1\n",
    "\n",
    "Use the `UnigramTagger` class and the `brown_train` object to create a unigram tagger.\n",
    "\n",
    "**Which tag is selecting to annotate each word?**\n",
    "\n",
    "**What's happening with unknown words?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training and test datasets\n",
    "from nltk import UnigramTagger\n",
    "\n",
    "# Train the unigram model\n",
    "\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.1.2\n",
    "\n",
    "Making use of the `evaluate` method measure how successful is this tagger.\n",
    "\n",
    "**Are we improving the performance of the tagger?**\n",
    "**Do your find the new performance sufficient enough for a NLP system?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.1.3\n",
    "\n",
    "If we analyze the tagger annotation, we will see that it assigns `None` to unknown words. As explained in class, a good way to improve this is to tag unknowns words as `NOUN` (the most common tag). This is known as a backoff tagger (i.e., a second tagger that applies where the original one cannot identify the tag for a word)\n",
    "\n",
    "NLTK provides a simple way to implement this backoff tagging. All the constructors for the Tagger classes (e.g., `UnigramTagger`) have a parameter `backoff` where you can set the backoff tagger that will apply. In this case, our backoff tagger will be the `DefaultTagger` that annotates `NOUN`, which we developed in the exercise below .\n",
    "\n",
    "**Using the `DefaultTagger` and `UnigramTagger` classes, create a tagger that assigns the most common tag to each word and, for unknown words, assigns a backoff tag of `NOUN`.**\n",
    "\n",
    "**What's the accuracy of this tagger? Do we improved our performance?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Using Morphological Clues\n",
    "\n",
    "As mentioned above, another knowledge source to perform tagging is to look at the letter structure of the words. \n",
    "We will look at 2 different methods to use this knowledge. \n",
    "First, we will use nltk.RegexpTagger to recognize specific regular expressions in words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regexp accuracy 48.2%\n"
     ]
    }
   ],
   "source": [
    "from nltk import RegexpTagger\n",
    "\n",
    "regexp_tagger = RegexpTagger(\n",
    "     [(r'^-?[0-9]+(.[0-9]+)?$', 'NUM'),   # cardinal numbers\n",
    "      (r'(The|the|A|a|An|an)$', 'DET'),   # articles\n",
    "      (r'.*able$', 'ADJ'),                # adjectives\n",
    "      (r'.*ness$', 'NOUN'),               # nouns formed from adjectives\n",
    "      (r'.*ly$', 'ADV'),                  # adverbs\n",
    "      (r'.*s$', 'NOUN'),                  # plural nouns\n",
    "      (r'.*ing$', 'VERB'),                # gerunds\n",
    "      (r'.*ed$', 'VERB'),                 # past tense verbs\n",
    "      (r'.*', 'NOUN')                     # nouns (default)\n",
    "])\n",
    "\n",
    "print('Regexp accuracy %4.1f%%' % (100.0 * regexp_tagger.evaluate(brown_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regular expressions are tested in order. If one matches, it decides the tag. Else it tries the next tag. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The question we face when we see such a \"rule-based\" tagger are:\n",
    "\n",
    "- How do we find the most successful regular expressions?\n",
    "- In which order should we try the regular expressions?\n",
    "\n",
    "A typical answer to such questions is: \n",
    "\n",
    "- let's learn these parameters from a training corpus. \n",
    "\n",
    "The `nltk.AffixTagger` is a trainable tagger that attempts to learn word patterns. \n",
    "It only looks at the last letters in the words in the training corpus, and counts how often a word suffix \n",
    "can predict the word tag. \n",
    "In other words, we only learn rules of the form ('.*xyz' , POS). \n",
    "This is how the affix tagger is used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Affix tagger accuracy: 42.3%\n"
     ]
    }
   ],
   "source": [
    "from nltk import AffixTagger\n",
    "\n",
    "affix_tagger = AffixTagger(brown_train, backoff=DefaultTagger('NOUN'))\n",
    "print('Affix tagger accuracy: %4.1f%%' % (100.0 * affix_tagger.evaluate(brown_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should we be disappointed that the \"data-based approach\" performs worse than the hand-written rules (42% vs. 48%)? \n",
    "\n",
    "Not necessarily: note that our hand-written rules include cases that the AffixTagger cannot learn - we match cardinal numbers and suffixes with more than 3 letters. \n",
    "\n",
    "Let us see whether the combination of the 2 taggers helps:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2.1\n",
    "\n",
    "**Using the `AffixTagger` class, creates a tagger that learns from word patterns and that uses the previous `RegexpTagger` as backoff.**\n",
    "\n",
    "**Evaluate and analyze the performance of the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2.2\n",
    "\n",
    "In the previous exercise we created an `AffixTagger` that is able to learn the annotation from the word patterns. Perhaps, we could apply this tagger to annotate the unknown words (i.e., to use it as a backoff tagger). In the previous section, we used a NOUN-default tagger for that. How much does this tagger help the `UnigramTagger` if we use it as a backoff instead of the NOUN-default tagger?\n",
    "\n",
    "**Use the `AffixTagger` that we created below as a backoff tagger for the `UnigramTagger` in the previous section**\n",
    "\n",
    "**Are we improving our tagger?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2.3 Looking at the Context\n",
    "\n",
    "At this point, we have combined 2 major sources of information: dictionary and morphology and obtained about 95.4% accuracy. The last source of knowledge we want to exploit the context of the word to be tagged: **the words that appear around the word to be tagged**. \n",
    "\n",
    "The intuition is that if we have to decide between `book` as a verb or a noun, the word/s preceding `book` can give us strong cues: for example, if it is an article (`the` or `a`) then we would be sure that `book` is a noun; if it is `to`, then we would be sure it is a verb.\n",
    "\n",
    "How can we turn this intuition into working code? The easiest way to detect predictive contexts is to construct a list of contexts - and for each context, keep track of the distribution of tags that follow it. Luckily for us, this procedure is already implemented into the `NgramTagger`, which takes a parameter a number setting the length of the context.\n",
    "\n",
    "As usual, if the tagger cannot make a decision (because the observed context was never seen at training time), \n",
    "the decision is delegated to a backoff tagger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.3.1\n",
    "\n",
    "**Use the `NgramTagger` to create a context-based tagger. For the cases that this tagger cannot annotate anything, use the previous `UnigramTagger` as backoff.**\n",
    "\n",
    "**Try different context sizes (you can set that as a parameter when you create the `NgramTagger`) and analyze how it affects to the final performance of the model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import NgramTagger\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This practice introduced tools to tag parts of speech in free text. The key point of the approach we investigated is that it is **data-driven**:\n",
    "\n",
    "- We first define possible knowledge sources that can help us solve the task. Specifically, we investigated \n",
    "  * dictionary, \n",
    "  * morphological \n",
    "  * context\n",
    "  as possible sources.\n",
    "\n",
    "- We tested simple machine learning methods: data is acquired by inspecting a training dataset, then evaluated by testing on a test dataset.\n",
    "\n",
    "- We investigated one method to combine several systems into a combined system: backoff models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Materials: Practical Tagging\n",
    "\n",
    "In this practice we have played with the development of new Taggers. You can refer back to this Notebook if and when you need to create your own Taggers. Nevertheless, most of the time the Taggers already included in the different libraries will do the trick for you.\n",
    "\n",
    "In particular, NLTK provides you a way to tag your dataset with just a couple of lines of code by using the `pos_tag` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need to do is to tokenize the sentence to be tagged. To that end, we can make use of the `word_tokenize` function in NLTK. To perform this tokenization, we need to download an already developed tokenizer model (`punkt`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\madcastea\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['And', 'now', 'for', 'something', 'completely', 'different']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = nltk.word_tokenize(\"And now for something completely different\")\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we should feed our tokenized text to the pos tagging function. In this case, we are going to apply one of the already pre-trained pos tagging models included into NLTK: `averaged_perceptron_tagger`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\madcastea\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('And', 'CC'),\n",
       " ('now', 'RB'),\n",
       " ('for', 'IN'),\n",
       " ('something', 'NN'),\n",
       " ('completely', 'RB'),\n",
       " ('different', 'JJ')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have more than one sentence to parse, we can make use of some of the Sentence Tokenizers that nltk provides (e.g. `sent_tokenize`) to split the text in sentences, and the the word tokenizer to split each sentence in words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences: ['And now for something completely different.', 'This is just another sentence']\n",
      "Text: [['And', 'now', 'for', 'something', 'completely', 'different', '.'], ['This', 'is', 'just', 'another', 'sentence']]\n",
      "\n",
      "Tagging: [('And', 'CC'), ('now', 'RB'), ('for', 'IN'), ('something', 'NN'), ('completely', 'RB'), ('different', 'JJ'), ('.', '.')]\n",
      "Tagging: [('This', 'DT'), ('is', 'VBZ'), ('just', 'RB'), ('another', 'DT'), ('sentence', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "sentences = nltk.sent_tokenize(\"And now for something completely different. This is just another sentence\")\n",
    "print(\"Sentences:\", sentences)\n",
    "text = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "print(\"Text:\", text)\n",
    "print()\n",
    "for tagging in [nltk.pos_tag(t) for t in text]:\n",
    "    print(\"Tagging:\",tagging)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see a full example with a proper corpus. NLTK provides many corpora that can be used for research or for the training of our NLP system. To find a comprehensive list of all the corpus and how to use them, please refer to the [2nd Chapter of the NLTK book](https://www.nltk.org/book/ch02.html).\n",
    "\n",
    "We will use the corpus `state_union` including the texts of the State of the Union addresses since 1945. Let's load one of these speeches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package state_union to\n",
      "[nltk_data]     C:\\Users\\madcastea\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\state_union.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"PRESIDENT HARRY S. TRUMAN'S MESSAGE TO THE CONGRESS ON THE STATE OF THE UNION AND ON THE BUDGET FOR 1946.\\n \\nJanuary 21, 1946. Dated January 14, 1946 \\n\\nTo the Congress of the United States:\\nA quarter century ago the Congress decided that it could no longer consider the financial programs of the various departments on a piecemeal basis. Instead it has called on the President to present a comprehensive Executive Budget. The Congress has shown its satisfaction with that method by extending the budget system and tightening its controls. The bigger and more complex the Federal Program, the more necessary it is for the Chief Executive to submit a single budget for action by the Congress.\\nAt the same time, it is clear that the budgetary program and the general program of the Government are actually inseparable. The President bears the responsibility for recommending to the Congress a comprehensive set of proposals on all Government activities and their financing. In formulating policies, as in\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download and import the State of the Union adresses corpus\n",
    "nltk.download('state_union')\n",
    "from nltk.corpus import state_union\n",
    "\n",
    "text = state_union.raw(\"1946-Truman.txt\")\n",
    "text[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define a function `tag_corpus` that takes care of the tagging process. First, it splits the text in sentences with the `sent_tokenize` function. Then, it iterates over these sentences, tokenize them with the `word_tokenize` function and apply the `pos_tag` function to the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: PRESIDENT HARRY S. TRUMAN'S MESSAGE TO THE CONGRESS ON THE STATE OF THE UNION AND ON THE BUDGET FOR 1946. \n",
      "Tagging: [('PRESIDENT', 'NNP'), ('HARRY', 'NNP'), ('S.', 'NNP'), ('TRUMAN', 'NNP'), (\"'S\", 'POS'), ('MESSAGE', 'NN'), ('TO', 'VBD'), ('THE', 'NNP'), ('CONGRESS', 'NNP'), ('ON', 'NNP'), ('THE', 'NNP'), ('STATE', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('UNION', 'NNP'), ('AND', 'NNP'), ('ON', 'NNP'), ('THE', 'NNP'), ('BUDGET', 'NNP'), ('FOR', 'NNP'), ('1946', 'CD'), ('.', '.')]\n",
      "\n",
      "Sentence: January 21, 1946. \n",
      "Tagging: [('January', 'NNP'), ('21', 'CD'), (',', ','), ('1946', 'CD'), ('.', '.')]\n",
      "\n",
      "Sentence: Dated January 14, 1946 \n",
      "\n",
      "To the Congress of the United States:\n",
      "A quarter century ago the Congress decided that it could no longer consider the financial programs of the various departments on a piecemeal basis. \n",
      "Tagging: [('Dated', 'VBN'), ('January', 'NNP'), ('14', 'CD'), (',', ','), ('1946', 'CD'), ('To', 'TO'), ('the', 'DT'), ('Congress', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('United', 'NNP'), ('States', 'NNPS'), (':', ':'), ('A', 'DT'), ('quarter', 'NN'), ('century', 'NN'), ('ago', 'IN'), ('the', 'DT'), ('Congress', 'NNP'), ('decided', 'VBD'), ('that', 'IN'), ('it', 'PRP'), ('could', 'MD'), ('no', 'RB'), ('longer', 'RB'), ('consider', 'VB'), ('the', 'DT'), ('financial', 'JJ'), ('programs', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('various', 'JJ'), ('departments', 'NNS'), ('on', 'IN'), ('a', 'DT'), ('piecemeal', 'JJ'), ('basis', 'NN'), ('.', '.')]\n",
      "\n",
      "Sentence: Instead it has called on the President to present a comprehensive Executive Budget. \n",
      "Tagging: [('Instead', 'RB'), ('it', 'PRP'), ('has', 'VBZ'), ('called', 'VBN'), ('on', 'IN'), ('the', 'DT'), ('President', 'NNP'), ('to', 'TO'), ('present', 'VB'), ('a', 'DT'), ('comprehensive', 'JJ'), ('Executive', 'NNP'), ('Budget', 'NNP'), ('.', '.')]\n",
      "\n",
      "Sentence: The Congress has shown its satisfaction with that method by extending the budget system and tightening its controls. \n",
      "Tagging: [('The', 'DT'), ('Congress', 'NNP'), ('has', 'VBZ'), ('shown', 'VBN'), ('its', 'PRP$'), ('satisfaction', 'NN'), ('with', 'IN'), ('that', 'DT'), ('method', 'NN'), ('by', 'IN'), ('extending', 'VBG'), ('the', 'DT'), ('budget', 'NN'), ('system', 'NN'), ('and', 'CC'), ('tightening', 'VBG'), ('its', 'PRP$'), ('controls', 'NNS'), ('.', '.')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def tag_corpus(corpus_text):\n",
    "    try:\n",
    "        for sentence in nltk.sent_tokenize(corpus_text)[:5]: # We just process 5 sentences for the sake of simplicity\n",
    "            words = nltk.word_tokenize(sentence)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            print(\"Sentence:\", sentence, \"\\nTagging:\", tagged)\n",
    "            print()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "\n",
    "tag_corpus(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same function in a more *pythonic* way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: PRESIDENT HARRY S. TRUMAN'S MESSAGE TO THE CONGRESS ON THE STATE OF THE UNION AND ON THE BUDGET FOR 1946. \n",
      "Tagging: [('PRESIDENT', 'NNP'), ('HARRY', 'NNP'), ('S.', 'NNP'), ('TRUMAN', 'NNP'), (\"'S\", 'POS'), ('MESSAGE', 'NN'), ('TO', 'VBD'), ('THE', 'NNP'), ('CONGRESS', 'NNP'), ('ON', 'NNP'), ('THE', 'NNP'), ('STATE', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('UNION', 'NNP'), ('AND', 'NNP'), ('ON', 'NNP'), ('THE', 'NNP'), ('BUDGET', 'NNP'), ('FOR', 'NNP'), ('1946', 'CD'), ('.', '.')] \n",
      "\n",
      "Sentence: January 21, 1946. \n",
      "Tagging: [('January', 'NNP'), ('21', 'CD'), (',', ','), ('1946', 'CD'), ('.', '.')] \n",
      "\n",
      "Sentence: Dated January 14, 1946 \n",
      "\n",
      "To the Congress of the United States:\n",
      "A quarter century ago the Congress decided that it could no longer consider the financial programs of the various departments on a piecemeal basis. \n",
      "Tagging: [('Dated', 'VBN'), ('January', 'NNP'), ('14', 'CD'), (',', ','), ('1946', 'CD'), ('To', 'TO'), ('the', 'DT'), ('Congress', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('United', 'NNP'), ('States', 'NNPS'), (':', ':'), ('A', 'DT'), ('quarter', 'NN'), ('century', 'NN'), ('ago', 'IN'), ('the', 'DT'), ('Congress', 'NNP'), ('decided', 'VBD'), ('that', 'IN'), ('it', 'PRP'), ('could', 'MD'), ('no', 'RB'), ('longer', 'RB'), ('consider', 'VB'), ('the', 'DT'), ('financial', 'JJ'), ('programs', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('various', 'JJ'), ('departments', 'NNS'), ('on', 'IN'), ('a', 'DT'), ('piecemeal', 'JJ'), ('basis', 'NN'), ('.', '.')] \n",
      "\n",
      "Sentence: Instead it has called on the President to present a comprehensive Executive Budget. \n",
      "Tagging: [('Instead', 'RB'), ('it', 'PRP'), ('has', 'VBZ'), ('called', 'VBN'), ('on', 'IN'), ('the', 'DT'), ('President', 'NNP'), ('to', 'TO'), ('present', 'VB'), ('a', 'DT'), ('comprehensive', 'JJ'), ('Executive', 'NNP'), ('Budget', 'NNP'), ('.', '.')] \n",
      "\n",
      "Sentence: The Congress has shown its satisfaction with that method by extending the budget system and tightening its controls. \n",
      "Tagging: [('The', 'DT'), ('Congress', 'NNP'), ('has', 'VBZ'), ('shown', 'VBN'), ('its', 'PRP$'), ('satisfaction', 'NN'), ('with', 'IN'), ('that', 'DT'), ('method', 'NN'), ('by', 'IN'), ('extending', 'VBG'), ('the', 'DT'), ('budget', 'NN'), ('system', 'NN'), ('and', 'CC'), ('tightening', 'VBG'), ('its', 'PRP$'), ('controls', 'NNS'), ('.', '.')] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def pythonized_tag_corpus(corpus_text):\n",
    "    try:\n",
    "        [print(\"Sentence:\", sentence, \"\\nTagging:\", nltk.pos_tag(nltk.word_tokenize(sentence)), \"\\n\") for sentence in nltk.sent_tokenize(corpus_text)[:5]]\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        \n",
    "pythonized_tag_corpus(text)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
